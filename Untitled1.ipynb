{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRqSQYlG2ZcG"
      },
      "source": [
        "# размер, к которому масштабируются изображения\n",
        "input_size = 224"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLPqmsDk_Dk3",
        "outputId": "7bf456c2-4c0f-4309-c811-464cf6397345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "\n",
        "from skimage import io\n",
        "import random\n",
        "\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from PIL import Image\n",
        "\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch Version:  1.3.1\n",
            "Torchvision Version:  0.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6f7DQYB3Frz"
      },
      "source": [
        "# библиотека для аугментаций\n",
        "from albumentations import (\n",
        "    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
        "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
        "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, ChannelShuffle, Cutout,\n",
        "    IAASharpen, IAAEmboss, RandomContrast, RandomBrightness, Flip, OneOf, Compose\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUzgyZNz_QKi"
      },
      "source": [
        "# Аугментирование и нормализация данных для тренировки\n",
        "# Просто нормализация для валидации\n",
        "def strong_aug(p=.5):\n",
        "    return Compose([\n",
        "        HorizontalFlip(),\n",
        "        OneOf([\n",
        "            IAAAdditiveGaussianNoise(),\n",
        "            GaussNoise(),\n",
        "        ], p=0.4),\n",
        "        OneOf([\n",
        "            MotionBlur(p=.2),\n",
        "            MedianBlur(blur_limit=3, p=.1),\n",
        "            Blur(blur_limit=3, p=.1),\n",
        "        ], p=0.3),\n",
        "        OneOf([\n",
        "            OpticalDistortion(p=0.3),\n",
        "            GridDistortion(p=.1),\n",
        "            IAAPiecewiseAffine(p=0.3),\n",
        "        ], p=0.2),\n",
        "        OneOf([\n",
        "            CLAHE(clip_limit=2),\n",
        "            IAASharpen(),\n",
        "            RandomContrast(),\n",
        "            RandomBrightness(),\n",
        "        ], p=0.3),\n",
        "        HueSaturationValue(p=0.3),\n",
        "        ChannelShuffle(),\n",
        "        Cutout(num_holes=20, max_h_size=16, max_w_size=16)\n",
        "    ], p=p)\n",
        "\n",
        "def augment(aug, image):\n",
        "    return aug(image=image)['image']\n",
        "\n",
        "class MyTransform(object):\n",
        "    def __call__(self, img):\n",
        "        aug = strong_aug(p=0.9)\n",
        "        return Image.fromarray(augment(aug, np.array(img)))\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        MyTransform(),\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBQo2376aCC9"
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORlFl_8k_njH",
        "outputId": "c2fee002-9d69-451f-bd03-187661c78cca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEqMrFhRHDdu"
      },
      "source": [
        "Данные лежат на гугл диске в том виде, в каком они были получены на kaggle - в виде архива.   \n",
        "\n",
        "(архив с папкой simpsons4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v3EDBHE-IEQ"
      },
      "source": [
        "!unzip -q /content/gdrive/My\\ Drive/simpsons4.zip -d data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYJu7vNv-eW3"
      },
      "source": [
        "train_dir = Path('data/train') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HahN4L_DLZp"
      },
      "source": [
        "test_dir = Path('data/testset/testset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTDKN4ubAdwY"
      },
      "source": [
        "data_dir = Path('data/') # для блока с сабмитом"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T6bIaxuJtAn"
      },
      "source": [
        "batch_size = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxzYKvec_1yy"
      },
      "source": [
        "# Create training and validation datasets\n",
        "image_datasets = {'train': datasets.ImageFolder(os.path.join(train_dir, 'simpsons_dataset'), data_transforms['train']), \n",
        "                  'val': datasets.ImageFolder(os.path.join(train_dir, 'simpsons_dataset'), data_transforms['val'])}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSUfF7VJAJWu"
      },
      "source": [
        "# Create training and validation dataloaders\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, \n",
        "                                                   shuffle=True, num_workers=4) for x in ['train','val']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-3evPOFAdZW",
        "outputId": "a5f1bc71-0c58-4de1-ecc9-a13da95d66c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "print('dataset_sizes: ', dataset_sizes)\n",
        "print(*class_names, sep='\\n')\n",
        "print(len(class_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset_sizes:  {'train': 20933, 'val': 20933}\n",
            "abraham_grampa_simpson\n",
            "agnes_skinner\n",
            "apu_nahasapeemapetilon\n",
            "barney_gumble\n",
            "bart_simpson\n",
            "carl_carlson\n",
            "charles_montgomery_burns\n",
            "chief_wiggum\n",
            "cletus_spuckler\n",
            "comic_book_guy\n",
            "disco_stu\n",
            "edna_krabappel\n",
            "fat_tony\n",
            "gil\n",
            "groundskeeper_willie\n",
            "homer_simpson\n",
            "kent_brockman\n",
            "krusty_the_clown\n",
            "lenny_leonard\n",
            "lionel_hutz\n",
            "lisa_simpson\n",
            "maggie_simpson\n",
            "marge_simpson\n",
            "martin_prince\n",
            "mayor_quimby\n",
            "milhouse_van_houten\n",
            "miss_hoover\n",
            "moe_szyslak\n",
            "ned_flanders\n",
            "nelson_muntz\n",
            "otto_mann\n",
            "patty_bouvier\n",
            "principal_skinner\n",
            "professor_john_frink\n",
            "rainier_wolfcastle\n",
            "ralph_wiggum\n",
            "selma_bouvier\n",
            "sideshow_bob\n",
            "sideshow_mel\n",
            "snake_jailbird\n",
            "troy_mcclure\n",
            "waylon_smithers\n",
            "42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7m_MEv_Akcb",
        "outputId": "0cea4d1d-6d86-42ad-fec3-cad91084a8e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdL3HY2yEGE2"
      },
      "source": [
        "Функция обучения модели с:\n",
        "1. планированием скорости обучения\n",
        "2. сохранением лучшей модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Na0N4EIND7vi"
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Каждая эпоха имеет фазу обучения и проверки\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Установить модель в режим обучения\n",
        "            else:\n",
        "                model.eval()   # Установить модель в режим вычисления\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict()) # сохраняем веса\n",
        "\n",
        "                torch.save(best_model_wts, \"/content/gdrive/My Drive/model_wights.pth\") # тут веса сохраняются на подключенный гугл диск.\n",
        "                \n",
        "\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6SzohOY-jMr"
      },
      "source": [
        "### Следующий выделенный блок нужен исключительно для создания сабмита."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZfrJ4BtYUw4"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZSVd8m3YYTQ"
      },
      "source": [
        "def predict(model, test_loader):\n",
        "    with torch.no_grad():\n",
        "        logits = []\n",
        "    \n",
        "        for inputs in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            model.eval()\n",
        "            outputs = model(inputs).cpu()\n",
        "            logits.append(outputs)\n",
        "            \n",
        "    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n",
        "    return probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ujwzk1a7ncj"
      },
      "source": [
        "class SimpsonsDataset(Dataset):\n",
        "  def __init__(self, files, mode):\n",
        "    super().__init__()\n",
        "    self.files = files\n",
        "    self.mode = mode\n",
        "\n",
        "    if self.mode not in DATA_MODES:\n",
        "      print(f'wrong mode: {self.mode}')\n",
        "      raise NameError\n",
        "\n",
        "    self.len_ = len(self.files)\n",
        "    self.label_encoder = LabelEncoder()\n",
        "\n",
        "    if self.mode != 'test':\n",
        "      self.labels = [path.parent.name for path in self.files]\n",
        "      self.label_encoder.fit(self.labels)\n",
        "\n",
        "      with open('label_encoder.pkl', 'wb') as le_dump:\n",
        "        pickle.dump(self.label_encoder, le_dump)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len_\n",
        "\n",
        "  def load_sample(self, file):\n",
        "    image = Image.open(file)\n",
        "    image.load()\n",
        "    return image\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])                                \n",
        "    ])\n",
        "   \n",
        "    x = self.load_sample(self.files[index])\n",
        "    x = self._prepare_sample(x)\n",
        "    x = np.array(x / 255, dtype='float32')\n",
        "\n",
        "    x = transform(x)\n",
        "    if self.mode == 'test':\n",
        "      return x\n",
        "    else:\n",
        "\n",
        "      label = self.labels[index]\n",
        "      label_id = self.label_encoder.transform([label])\n",
        "      y = label_id.item()\n",
        "      return x, y\n",
        "\n",
        "  def _prepare_sample(self, image):\n",
        "    image = image.resize((RESCALE_SIZE, RESCALE_SIZE))\n",
        "    return np.array(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxCHRsXW7x9S"
      },
      "source": [
        "# разные режимы датасета \n",
        "DATA_MODES = ['train', 'val', 'test']\n",
        "# все изображения будут масштабированы к размеру 224x224 px\n",
        "RESCALE_SIZE = input_size\n",
        "# работаем на видеокарте\n",
        "DEVICE  = torch.device(\"cuda\")\n",
        "\n",
        "TRAIN_DIR = Path('data/train/simpsons_dataset')\n",
        "TEST_DIR = Path('data/testset/testset')\n",
        "\n",
        "train_val_files = sorted(list(TRAIN_DIR.rglob('*.jpg')))\n",
        "test_files = sorted(list(TEST_DIR.rglob('*.jpg')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm6c7JVP8Kb8"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_val_labels = [path.parent.name for path in train_val_files]\n",
        "train_files, val_files = train_test_split(train_val_files, test_size=0.3, \\\n",
        "                                          stratify=train_val_labels)\n",
        "val_dataset = SimpsonsDataset(val_files, mode='val')\n",
        "train_dataset = SimpsonsDataset(train_files, mode='train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFfb63jM6SOT"
      },
      "source": [
        "(Да, так не очень хорошо делать)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzQUb0RJY8tr"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSbr53kLGeqa"
      },
      "source": [
        "Загружаем проедварительно обученную нейронную сеть и сбрасываем последний полносвязный слой"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TlEIf7aFYpa",
        "outputId": "ac268230-0864-4de8-8df3-c8b8f9e82d15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "model_ft = models.densenet121(pretrained=True)\n",
        "#num_ftrs = model_ft.fc.in_features # для resnet\n",
        "\n",
        "#model_ft.fc = nn.Linear(num_ftrs, len(class_names)) # для resnet\n",
        "model_ft.classifier = nn.Linear(1024, len(class_names))\n",
        "\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/checkpoints/densenet121-a639ec97.pth\n",
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 62.3MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FZlX8zkG1M1"
      },
      "source": [
        "### Тренировка и оценивание модели\n",
        "Число эпох - 30 (примерно, но не точно). Больше не пробовал из-за долгого обучения сети (densenet), но и точного кол-ва эпох не помню, т.к. из-за долгой тренировки была ошибка к доступу gpu, и спасли лишь сохраненные значения весов (тогда я их сохранял не на гугл-диск, а вручную), ну и на какой эпохе было сохранение я не запомнил, и на какой эпохе был сбой - тоже.\n",
        "\n",
        "С сетью из ноутбука (sipmle cnn) у меня большой точности не вышло, но кроме изменения кол-ва эпох (вроде больше 30-40 я не ставил) я ничего больше не менял.\n",
        "\n",
        "Потом я попробовал что-то поделать с ноутбуком из kaggle (\n",
        "baseline with submission), добавить туда аугментации (не вышло), потренировать на большем кол-ве эпох (результат ~0.94). Затем решил подключить предобученную нейросеть (vgg16, densenet121), но эты вещи не работали с adam'ом (значения лосс и acc были nan), заменил его на sgd. vgg16 стал сразу выдавать значения лоссов и acc - nan, а densenet сумел доучиться до ~0,85. Я попробовал поменять трейн функцию (добавил туда model.train()), и странно, но за первую эпоху модель выдала скор ~0,94 (0,96 приватный), затем шли снова nan значения в loss и acc.  \n",
        "Я взял функцию трейн (вместе со всем остальным) отсюда    \n",
        "\n",
        " https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html  \n",
        "\n",
        "и отсюда   \n",
        "\n",
        "https://pytorch.org/tutorials/beginner/transfer_learning_tutorial    \n",
        "\n",
        "С ними все пошло хорошо. Потренировал сети resnet18 (скор больше 0.95) и resnet50 (скор был выше чем у resnet18 (почти 0.99) при одинаковом кол-ве эпох (вроде 25)). Затем я пробовал подключить inception_v3, но, видимо, кроме инструкций из туториала (1 ссылка) нужно было изменять еще какие-то слои, но времени было мало разбираться, и я снова взял denenet121, добавил аугментаций (mytransform), обучил и получил текущий результат.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wP2rYH7KG3wN",
        "outputId": "6cec9123-5db3-45a5-c644-28f3a769c5f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "model_ft, hist = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 0.3479 Acc: 0.9087\n",
            "val Loss: 0.0202 Acc: 0.9957\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 0.3327 Acc: 0.9139\n",
            "val Loss: 0.0187 Acc: 0.9957\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 0.3356 Acc: 0.9125\n",
            "val Loss: 0.0175 Acc: 0.9961\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 0.3234 Acc: 0.9147\n",
            "val Loss: 0.0175 Acc: 0.9962\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 0.3287 Acc: 0.9112\n",
            "val Loss: 0.0172 Acc: 0.9965\n",
            "\n",
            "Training complete in 62m 50s\n",
            "Best val Acc: 0.996465\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOR8xqbRGZ5h"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ldPgByMGOdo"
      },
      "source": [
        "Все, что ниже - для сабмита"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5D5bVJoGTC-"
      },
      "source": [
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olsb4ALLbM33"
      },
      "source": [
        "label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))\n",
        "\n",
        "test_dataset = SimpsonsDataset(test_files, mode=\"test\")\n",
        "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=64)\n",
        "probs = predict(model_ft, test_loader)\n",
        "\n",
        "preds = label_encoder.inverse_transform(np.argmax(probs, axis=1))\n",
        "test_filenames = [path.name for path in test_dataset.files]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HMrNtj39B9H",
        "outputId": "4e0cb5e8-f5be-44a2-eb38-46fd7288ba29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import pandas as pd\n",
        "submit = pd.DataFrame({'Id': test_filenames, 'Expected': preds})\n",
        "submit.info()\n",
        "from google.colab import files\n",
        "\n",
        "submit.to_csv('submit9.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 991 entries, 0 to 990\n",
            "Data columns (total 2 columns):\n",
            "Id          991 non-null object\n",
            "Expected    991 non-null object\n",
            "dtypes: object(2)\n",
            "memory usage: 15.6+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}